{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4ac3f-9605-43bb-ad95-f49e59b9b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, avg, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe028371-ce0f-44c6-8e55-2a3f7a105dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визначення структур даних для JSON\n",
    "schema_athlete_bio = StructType([\n",
    "    StructField(\"athlete_id\", IntegerType(), True),\n",
    "    StructField(\"height\", FloatType(), True),\n",
    "    StructField(\"weight\", FloatType(), True),\n",
    "    StructField(\"sex\", StringType(), True),\n",
    "    StructField(\"country_noc\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema_event_results = StructType([\n",
    "    StructField(\"event_id\", IntegerType(), True),\n",
    "    StructField(\"athlete_id\", IntegerType(), True),\n",
    "    StructField(\"medal\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6af9fb-eea1-4bef-be0e-d5eb1b8c87c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Частина 1. \n",
    "# Створення стримуючої архітектури\n",
    "if __name__ == \"__main__\":\n",
    "    # Ініціалізація Spark сесії\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"StreamingPipeline\") \\\n",
    "        .master(\"spark://217.61.58.159:7077\") \\\n",
    "        .config(\"spark.ui.enabled\", \"true\") \\\n",
    "        .config(\"spark.ui.port\", \"8080\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151dbb8-24ab-40af-862f-1d9e57a94cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зчитуємо дані з MySQL для таблиці athlete_bio\n",
    "    df_athlete_bio = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:mysql://217.61.57.46:3306/neo_data\") \\\n",
    "        .option(\"dbtable\", \"athlete_bio\") \\\n",
    "        .option(\"user\", \"neo_data_admin\") \\\n",
    "        .option(\"password\", \"Proyahaxuqithab9oplp\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e5185e-49d0-4bee-a84e-e110412bc2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фільтрація записів із неповними даними\n",
    "    df_athlete_bio = df_athlete_bio.filter(\n",
    "        col(\"height\").isNotNull() & col(\"weight\").isNotNull()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d6e50f-bfdc-4803-9c98-176d78a94539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зчитуємо дані з Kafka топіка\n",
    "    df_event_results = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"77.81.230.104:9092\") \\\n",
    "        .option(\"subscribe\", \"athlete_event_results\") \\\n",
    "        .option(\"kafka.security.protocol\", \"PLAINTEXT\") \\\n",
    "        .option(\"kafka.sasl.jaas.config\", \n",
    "                f\"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"admin\\\" password=\\\"VawEzo1ikLtrA8Ug8THa\\\";\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe4c903-614f-4de2-bb2f-d17e33a5dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Декодування JSON даних із Kafka\n",
    "    df_event_results = df_event_results.selectExpr(\"CAST(value AS STRING)\")\n",
    "    df_event_results = df_event_results.withColumn(\"value\", from_json(col(\"value\"), schema_event_results))\n",
    "    df_event_results = df_event_results.select(\"value.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bac971-39c2-4b33-b702-9f3954ea5504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Злиття даних із athlete_bio та athlete_event_results\n",
    "combined_df = df_athlete_bio.join(df_event_results, on=\"athlete_id\", how=\"inner\")\n",
    "\n",
    "# Запис результатів у Kafka топік\n",
    "    query = combined_df.writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"77.81.230.104:9092\") \\\n",
    "        .option(\"topic\", \"processed_athlete_data\") \\\n",
    "        .start()\n",
    "\n",
    "    query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415967f-7fd4-4380-abee-5b8c85eaeb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Частина 2. Створення Batch Data Lake:\n",
    "if __name__ == \"__main__\":\n",
    "    # Ініціалізація Spark сесії\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"BatchDataLake\") \\\n",
    "        .master(\"spark://217.61.58.159:7077\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb4461-25c4-432e-a287-4ac2ad92e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завантаження файлів з FTP серверу\n",
    "url = \"https://ftp.goit.study/neoversity/\"\n",
    "    files = [\"athlete_bio.txt\", \"athlete_event_results.txt\"]\n",
    "\n",
    "    for file in files:\n",
    "        local_file_path = f\"bronze/{file}\"\n",
    "        full_url = url + file\n",
    "        print(f\"Downloading {file} from {full_url}\")\n",
    "\n",
    "        # Зчитування файлу та збереження у форматі Parquet\n",
    "        df = spark.read.option(\"header\", True).csv(full_url)\n",
    "        df.write.parquet(local_file_path)\n",
    "\n",
    "        print(f\"File {file} saved as Parquet in bronze layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca871a70-4e76-4fc2-b986-111f62163666",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Зчитуємо таблиці з бронзою\n",
    "    bronze_athlete_bio = spark.read.parquet(\"bronze/athlete_bio.txt\")\n",
    "    bronze_event_results = spark.read.parquet(\"bronze/athlete_event_results.txt\")\n",
    "\n",
    "# Очищення даних та видалення дублювань\n",
    "    cleaned_athlete_bio = bronze_athlete_bio.dropDuplicates()\n",
    "    cleaned_event_results = bronze_event_results.dropDuplicates()\n",
    "\n",
    "# Запис у срібну\n",
    "    cleaned_athlete_bio.write.parquet(\"silver/athlete_bio_silver.txt\")\n",
    "    cleaned_event_results.write.parquet(\"silver/athlete_event_results_silver.txt\")\n",
    "\n",
    "    print(\"Data cleaned and saved to silver layer.\")\n",
    "\n",
    "# Зчитуємо таблиці зі срібною\n",
    "    silver_athlete_bio = spark.read.parquet(\"silver/athlete_bio_silver.txt\")\n",
    "    silver_event_results = spark.read.parquet(\"silver/athlete_event_results_silver.txt\")\n",
    "\n",
    "# Об'єднуєнмо данні\n",
    "    joined_df = silver_athlete_bio.join(silver_event_results, on=\"athlete_id\", how=\"inner\")\n",
    "\n",
    "# Обчислення середніх значень weight і height\n",
    "    avg_stats_df = joined_df.groupBy(\"sex\", \"country_noc\") \\\n",
    "                            .agg(\n",
    "                                avg(\"weight\").alias(\"avg_weight\"),\n",
    "                                avg(\"height\").alias(\"avg_height\")\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b046c1f-53f3-464e-b62e-f8dc9259ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Додати час виконання\n",
    "    avg_stats_df = avg_stats_df.withColumn(\"timestamp\", current_timestamp())\n",
    "\n",
    "# Запис у золотий шар\n",
    "    avg_stats_df.write.parquet(\"gold/avg_stats\")\n",
    "\n",
    "    print(\"Processed data saved to gold layer.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
